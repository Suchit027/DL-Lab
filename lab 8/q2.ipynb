{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './names'\n",
    "files = [file for file in os.listdir(path)]\n",
    "vocabulary = set()\n",
    "for file in files:\n",
    "    with open(os.path.join(path, file), 'r', encoding= 'utf-8') as f:\n",
    "        for word in f.readlines():\n",
    "            for char in list(word.strip()):\n",
    "                vocabulary.add(char)\n",
    "        \n",
    "vocabulary = list(vocabulary)\n",
    "input_dim = len(vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, files, vocabulary, input_dim):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.vocabulary, self.input_dim = vocabulary, input_dim\n",
    "        self.word_tensor, self.word_data, self.label_index, self.label = [], [], [], []\n",
    "\n",
    "        for i in range(len(self.files)):\n",
    "            with open(os.path.join(path, self.files[i]), 'r', encoding= 'utf-8') as f:\n",
    "                for word in f.readlines():\n",
    "                    self.word_tensor.append(self.onehotencoding(word.strip()))\n",
    "                    self.label_index.append(torch.tensor(i))\n",
    "                    self.word_data.append(word.strip())\n",
    "                    self.label.append(self.files[i])\n",
    "\n",
    "    def onehotencoding(self, word):\n",
    "        ret = torch.zeros((len(word), 1, self.input_dim))\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in self.vocabulary:\n",
    "                ret[i, 0, self.vocabulary.index(word[i])] = 1\n",
    "            else:\n",
    "                ret[i, 0, -1] = 1\n",
    "        return ret.reshape((1, len(word), -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.word_tensor[index], self.word_data[index], self.label_index[index], self.label[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = Data('./names/', files, vocabulary, input_dim)\n",
    "# note\n",
    "train, test = torch.utils.data.random_split(alldata, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_features):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size= input_dim, hidden_size= 128, num_layers= 1, batch_first= True)\n",
    "        self.fc1 = torch.nn.Linear(in_features= 128, out_features= out_features)\n",
    "    def forward(self, x):\n",
    "        y, h = self.rnn(x)\n",
    "        y = y[:, -1, :]\n",
    "        return self.fc1(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(input_dim= input_dim, out_features= len(files))\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001, weight_decay= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, loss = 20936.30409527826\n",
      "epoch - 1, loss = 17121.894589433083\n",
      "epoch - 2, loss = 15585.996399236436\n",
      "epoch - 3, loss = 14947.810268568115\n",
      "epoch - 4, loss = 14443.727006836383\n",
      "epoch - 5, loss = 14458.36654445478\n",
      "epoch - 6, loss = 14252.085662359408\n",
      "epoch - 7, loss = 14193.348772683756\n",
      "epoch - 8, loss = 14351.90128091197\n",
      "epoch - 9, loss = 14551.83564152757\n",
      "epoch - 10, loss = 14566.513487397206\n",
      "epoch - 11, loss = 14684.958279825249\n",
      "epoch - 12, loss = 15087.16189686989\n",
      "epoch - 13, loss = 15087.881853227478\n",
      "epoch - 14, loss = 15186.075643055905\n",
      "epoch - 15, loss = 15184.144665243104\n",
      "epoch - 16, loss = 15637.481440077849\n",
      "epoch - 17, loss = 15384.58089388015\n",
      "epoch - 18, loss = 15450.231729058905\n",
      "epoch - 19, loss = 15547.618483333412\n",
      "epoch - 20, loss = 15480.739470179113\n",
      "epoch - 21, loss = 15749.851882539904\n",
      "epoch - 22, loss = 15916.2795302523\n",
      "epoch - 23, loss = 15966.253389358142\n",
      "epoch - 24, loss = 15615.821221468956\n",
      "epoch - 25, loss = 15893.593759628537\n",
      "epoch - 26, loss = 15888.182941966712\n",
      "epoch - 27, loss = 15860.644796686745\n",
      "epoch - 28, loss = 16139.34484842851\n",
      "epoch - 29, loss = 16054.131520928655\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(30):\n",
    "    batches = list(range(len(train)))\n",
    "    # note\n",
    "    random.shuffle(batches)\n",
    "    # note\n",
    "    batches = np.array_split(batches, len(batches) // 64)\n",
    "    running_loss = 0.0\n",
    "    for batch in batches:\n",
    "        for i in batch:\n",
    "            optimizer.zero_grad()\n",
    "            word_tensor, word_data, label_index, label = train[i]\n",
    "            word_tensor, label_index = word_tensor.to('cuda'), label_index.unsqueeze(0).to('cuda')\n",
    "            output = model(word_tensor)\n",
    "            loss = criterion(output, label_index)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "    print(f'epoch - {epoch}, loss = {running_loss}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_pred, all_label = [], []\n",
    "with torch.no_grad():\n",
    "    batches = list(range(len(test)))\n",
    "    random.shuffle(batches)\n",
    "    batches = np.array_split(batches, len(batches) // 64)\n",
    "    for batch in batches:\n",
    "        for i in batch:\n",
    "            word_tensor, word_data, label_index, label = test[i]\n",
    "            word_tensor = word_tensor.to('cuda')\n",
    "            output = model(word_tensor)\n",
    "            val, index = torch.max(output, dim = 1)\n",
    "            all_pred.append(index.to('cpu').numpy())\n",
    "            all_label.append(label_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7249626307922272\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(all_pred, all_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arabic.txt\n"
     ]
    }
   ],
   "source": [
    "word = 'Sharma'\n",
    "onehotword = torch.zeros((len(word), 1, input_dim))\n",
    "for i in range(len(word)):\n",
    "    if word[i] in vocabulary:\n",
    "        onehotword[i, 0, vocabulary.index(word[i])] = 1.0\n",
    "    else:\n",
    "        onehotword[i, 0, -1] = 1.0\n",
    "onehotword = onehotword.reshape((1, len(word), input_dim))\n",
    "model.eval()\n",
    "onehotword = onehotword.to('cuda')\n",
    "output = model(onehotword)\n",
    "val, ind = torch.max(output, dim = 1)\n",
    "answer = files[ind]\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
