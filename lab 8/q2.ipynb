{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './names'\n",
    "files = [file for file in os.listdir(path)]\n",
    "vocabulary = set()\n",
    "for file in files:\n",
    "    with open(os.path.join(path, file), 'r', encoding= 'utf-8') as f:\n",
    "        for word in f.readlines():\n",
    "            for char in list(word.strip()):\n",
    "                vocabulary.add(char)\n",
    "        \n",
    "vocabulary = list(vocabulary)\n",
    "input_dim = len(vocabulary) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(torch.utils.data.Dataset):\n",
    "    def __init__(self, path, files, vocabulary, input_dim):\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.vocabulary, self.input_dim = vocabulary, input_dim\n",
    "        self.word_tensor, self.word_data, self.label_index, self.label = [], [], [], []\n",
    "\n",
    "        for i in range(len(self.files)):\n",
    "            with open(os.path.join(path, self.files[i]), 'r', encoding= 'utf-8') as f:\n",
    "                for word in f.readlines():\n",
    "                    self.word_tensor.append(self.onehotencoding(word.strip()))\n",
    "                    self.label_index.append(torch.tensor(i))\n",
    "                    self.word_data.append(word.strip())\n",
    "                    self.label.append(self.files[i])\n",
    "\n",
    "    def onehotencoding(self, word):\n",
    "        ret = torch.zeros((len(word), 1, self.input_dim))\n",
    "        for i in range(len(word)):\n",
    "            if word[i] in self.vocabulary:\n",
    "                ret[i, 0, self.vocabulary.index(word[i])] = 1\n",
    "            else:\n",
    "                ret[i, 0, -1] = 1\n",
    "        return ret.reshape((1, len(word), -1))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.word_tensor[index], self.word_data[index], self.label_index[index], self.label[index]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = Data('./names/', files, vocabulary, input_dim)\n",
    "train, test = torch.utils.data.random_split(alldata, [0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, input_dim, out_features):\n",
    "        super().__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size= input_dim, hidden_size= 128, num_layers= 1, batch_first= True)\n",
    "        self.fc1 = torch.nn.Linear(in_features= 128, out_features= out_features)\n",
    "    def forward(self, x):\n",
    "        y, h = self.rnn(x)\n",
    "        y = y[:, -1, :]\n",
    "        return self.fc1(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network(input_dim= input_dim, out_features= len(files))\n",
    "model = model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001, weight_decay= 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch - 0, loss = 20651.485175973125\n",
      "epoch - 1, loss = 17071.902968951792\n",
      "epoch - 2, loss = 15635.974860505376\n",
      "epoch - 3, loss = 14894.697162134587\n",
      "epoch - 4, loss = 14526.435984684882\n",
      "epoch - 5, loss = 14603.666376518519\n",
      "epoch - 6, loss = 14480.88834379756\n",
      "epoch - 7, loss = 14492.082215148665\n",
      "epoch - 8, loss = 14646.508340227057\n",
      "epoch - 9, loss = 14892.29424189874\n",
      "epoch - 10, loss = 14667.916275638116\n",
      "epoch - 11, loss = 14938.64810840923\n",
      "epoch - 12, loss = 14806.568407434539\n",
      "epoch - 13, loss = 15130.981915629694\n",
      "epoch - 14, loss = 15227.648713134313\n",
      "epoch - 15, loss = 15469.241608211883\n",
      "epoch - 16, loss = 15605.151804631336\n",
      "epoch - 17, loss = 15892.293474008373\n",
      "epoch - 18, loss = 16196.0603591256\n",
      "epoch - 19, loss = 16371.782870057286\n",
      "epoch - 20, loss = 16608.052628370773\n",
      "epoch - 21, loss = 16489.02255625074\n",
      "epoch - 22, loss = 16530.51069921702\n",
      "epoch - 23, loss = 16831.283150280637\n",
      "epoch - 24, loss = 16910.571762392086\n",
      "epoch - 25, loss = 16940.035541308163\n",
      "epoch - 26, loss = 17103.900678068152\n",
      "epoch - 27, loss = 17009.19257538006\n",
      "epoch - 28, loss = 17124.132438642395\n",
      "epoch - 29, loss = 16838.14991026122\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(30):\n",
    "    batches = list(range(len(train)))\n",
    "    random.shuffle(batches)\n",
    "    batches = np.array_split(batches, len(batches) // 64)\n",
    "    running_loss = 0.0\n",
    "    for batch in batches:\n",
    "        for i in batch:\n",
    "            optimizer.zero_grad()\n",
    "            word_tensor, word_data, label_index, label = train[i]\n",
    "            word_tensor, label_index = word_tensor.to('cuda'), label_index.unsqueeze(0).to('cuda')\n",
    "            output = model(word_tensor)\n",
    "            loss = criterion(output, label_index)\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "    print(f'epoch - {epoch}, loss = {running_loss}')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_pred, all_label = [], []\n",
    "with torch.no_grad():\n",
    "    batches = list(range(len(test)))\n",
    "    random.shuffle(batches)\n",
    "    batches = np.array_split(batches, len(batches) // 64)\n",
    "    for batch in batches:\n",
    "        for i in batch:\n",
    "            word_tensor, word_data, label_index, label = test[i]\n",
    "            word_tensor = word_tensor.to('cuda')\n",
    "            output = model(word_tensor)\n",
    "            val, index = torch.max(output, dim = 1)\n",
    "            all_pred.append(index.to('cpu').numpy())\n",
    "            all_label.append(label_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7140009965122073\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(all_pred, all_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
